{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOpK1kN8sb43v0XIQj/V6ug"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [개념 필사] - 딥러닝 파이토치"
      ],
      "metadata": {
        "id": "HFfr63MSofnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CHAPTER 04 <딥러닝 시작>\n",
        "\n",
        "## **4.1 인공 신경망의 한계와 딥러닝 출현**\n",
        "\n",
        "◼ **퍼셉트론**: 입력층, 출력층, 가중치로 구성된 구조. 딥러닝(신경망)의 기원이 되는 알고리즘. 다수의 신호를 입력으로 받아 하나의 신호 출력.\n",
        "\n",
        "* AND 게이트: 모든 입력이 '1'일 때 작동\n",
        "* OR 게이트: 하나라도 '1'일 때 작동\n",
        "* XOR 게이트: 배타적 논리합. 한 개만 '1'일 때 작동 / ❗비선형적인 데이터 분리. 단층 퍼셉트론에서 학습 불가능. ▶ **다층 퍼셉트론** (입력층, 출력층 사이 하나 이상의 은닉층 배치)  \n",
        "\n",
        "   -- **심층 신경망: 은닉층 여러개 = 딥러닝**\n",
        "\n",
        "## **4.2 딥러닝 구조**\n",
        "\n",
        ">  ## 딥러닝 용어\n",
        "\n",
        "◼ **층**\n",
        "\n",
        "* 입력층: 데이터를 받아들이는 층\n",
        "* 은닉층: 모든 입력 노드부터 입력값을 받아 가중합 계산, 활성화 함수에 적용하여 출력층에 전달\n",
        "* 출력층: 신경망의 최종 결괏값이 포함된 층\n",
        "\n",
        "\n",
        "\n",
        "◼ **가중합(전달 함수)** : 각 노드에서 들어오는 신호에 가중치를 곱해 모두 더한 합계. 계산 후, 값을 활성화 함수로 전달.\n",
        "\n",
        "* **가중치** : 입력 값이 연산 결과에 미치는 영향력을 조절하는 요소. 노드 사이의 연결 강도.\n",
        "\n",
        "* **바이어스** : 가중합에 더해지는 상수. 하나의 뉴런에서 활성화 함수를 거쳐 최종출력되는 값을 조절.\n",
        "\n",
        "◼ **활성화 함수** : 전달함수에서 받은 값을 일정 기준에 따라 출력 값을 변화시키는 비선형 함수.\n",
        "\n",
        "* 시그모이드 함수: 선형 함수 결과를 0~1로 변형. 로지스틱. 기울기 소멸 문제로 인해 사용량 감소\n",
        "* 하이퍼볼릭 탄젠트 함수: 선형 함수 결과를 -1 ~ 1로 변형. 기울기 소멸 문제 존재.\n",
        "* 렐루 함수: 입력이 음수면 0, 양수면 입력값 출력. 속도 빠름. 기울기 소멸 문제 해결. 주로 은닉층에서 사용.❗ 0 출력으로 인한 학습 능력 감소\n",
        "* 리키 렐루 함수: 입력이 음수일 때 0.001 같은 매우 작은 수 반환 (렐루 함수 문제 해결)\n",
        "* 소프트맥스 함수: 입력값을 0~1 사이에 출력되도록 정규화. 총합이 1. 주로 출력 노드에서 사용. (개별 입력 신호의 지수 함수) ÷ (모든 입력 신호의 지수 함수합)\n",
        "\n",
        "◼ **손실 함수**: 가중치 학습을 위해 출력 함수와 실제 값 간의 오차를 측정하는 함수. 0에 가까울수록 좋음.\n",
        "\n",
        "* 평균 제곱 오차: 실제값과 예측값의 차이를 제곱하여 평균낸 것. MSE\n",
        "* 크로스 엔트로피 오차(CEE): 원-핫 인코딩 했을 때 사용 가능. 시그모이드의 영향을 덜 받으며 속도 빠름.\n",
        "\n",
        ">  ## 딥러닝 학습\n",
        "\n",
        "◼ **순전파**: 데이터를 기반으로 예측값을 계산하기 위해 전체 신경망으로 교차해 지나감. 모든 계산 완료 후 출력층 도달. 손실 함수로 손실 계산.\n",
        "\n",
        "◼ **역전파**: 손실 계산 후, 그 정보가 역으로 전파. 계산 완료된 값을 다시 순전파의 가중치값으로 사용.\n",
        "\n",
        "> ## 딥러닝 문제점, 해결 방안, 이점\n",
        "\n",
        "❗과적합 문제: 은닉층이 많을수록 예측 잘되지만, 훈련데이터를 과하게 학습. ▶ 드롭아웃 (임의의 일부 노드들을 제외)\n",
        "\n",
        "❗기울기 소멸 문제: 은닉층이 많은 신경망에서, 출력층에서 은닉층으로 전달되는 오차가 크게 줄어들어 학습이 되지 않는 현상. ▶ 렐루 함수 사용 (시그모이드, 하이퍼볼릭 탄젠트 x)\n",
        "\n",
        "❗성능 저하 문제: 경사 하강법에서 발생. ▶ 확률적 경사 하강법, 미니 배치 경사 하강법\n",
        "\n",
        "✨ 특성 추출: 데이터별 특징을 찾고, 벡터 변환하는 작업. 특성 추출 과정이 알고리즘에 통합됨. (전문지식 필요x)\n",
        "\n",
        "✨ 빅데이터 효율적 활용: 빅데이터로 인해 특성 추출이 가능해짐.\n",
        "\n",
        "\n",
        "## **4.3 딥러닝 알고리즘**\n",
        "\n",
        "✅ 딥러닝 알고리즘은 심층 신경망을 사용.\n",
        "\n",
        "✅ 심층 신경망: 입력층과 출력층 사이 다수의 은닉층을 포함하는 인공 신경망. 쉽게 비선형 분류 가능.\n",
        "\n",
        "✅ CNN, RNN 제일 많이 사용.\n",
        "\n",
        "**(1) 합성곱 신경망(CNN)**: 합성곱층과 풀링층을 포함하는 이미지 처리 성능이 좋은 인공 신경망 알고리즘. 이미지에서 객체 탐색. 패턴 찾기\n",
        "\n",
        "* 각 층의 입출력 형상 유지\n",
        "* 이미지 공간 정보 유지, 인접이미지와 차이가 있는 특징을 효과적으로 인식.\n",
        "* 복수 필터로 이미지의 특징 추출, 학습\n",
        "* 추출한 이미지 특징 모으고 강화하는 풀링층 존재\n",
        "* 필터를 공유 파라미터로 사용하므로 학습 파라미터 적음.\n",
        "\n",
        "**(2) 순환 신경망(RNN)**: 시계열 데이터를 학습하기 위한 인공 신경망.\n",
        "\n",
        "* 시간성을 가진 데이터 많음\n",
        "* 시간성 정보 이용\n",
        "* 시간에 따라 내용이 변하므로 동적이고 길이가 가변적인 데이터.\n",
        "* 매우 긴 데이터 처리\n",
        "* ❗기울기 소멸 문제 ▶ LSTM (메모리 개념 도입)\n",
        "* 자연어 처리에 좋음 - 언어모델링, 텍스트 생성, 자동번역, 음성 인식, 이미지캡션 생성\n",
        "\n",
        "**(3) 제한된 볼츠만 머신**: 가시층과 은닉층으로 구성된 모델. 가시층-은닉층 연결만 존재.\n",
        "\n",
        "* 차원 감소, 분류, 선경회귀, 협업 필터링, 특성값학습, 주제모델링\n",
        "* 기울기 소멸 문제 해결을 위한 사전 학습 용도로 활용 가능.\n",
        "* 심층 신뢰 신경망의 요소로 활용\n",
        "\n",
        "\n",
        "**(4) 심층 신뢰 신경망(DBN)**: 제한된 볼츠만 머신을 여러 층으로 쌓은 형태로 연결된 신경망. 비지도학습 가능. 일반화, 추상화 과정에 유용.\n",
        "\n",
        "* 제한된 볼츠만 머신 사전 훈련 ▶ 첫번째 층 고정, 두번째 층 사전 훈련 ▶ 원하는 만큼 쌓아올려 완성\n",
        "* 계층적 구조 생성\n",
        "* 추상적 특징 추출\n",
        "* 학습된 가중치를 다층 퍼셉트론의 가중치 초깃값으로 사용\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h95orlqcnrob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [코드 필사]"
      ],
      "metadata": {
        "id": "jnFwpXc_ofsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "G6ZjzXVDKTrw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 렐루, 소프트맥스 구현\n",
        "# 렐루 함수와 소프트 맥수 함수 파이토치에서 구현\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, n_feature, n_hidden, n_output):\n",
        "    super(Net, self).__init__()\n",
        "    self.hidden = torch.nn.Linear(n_feature, n_hidden) # 은닉층\n",
        "    self.relu = torch.nn.ReLu(inplace=True)\n",
        "    self.out = torch.nn.Lienar(n_hidden, n_output) # 출력층\n",
        "  def forward(self, x):\n",
        "    x = self.hidden(x)\n",
        "    x = self.relu(x) # 은닉층을 위한 활성화 함수\n",
        "    x = self.out(x)\n",
        "    x = self.softmax(x) # 출력층을 위한 소프트맥스 활성화 함수\n",
        "    return x"
      ],
      "metadata": {
        "id": "gmG55TeCKDGB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MSE\n",
        "#loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
        "#y_pred  = model(x)\n",
        "#loss = loss_fn(y_pred, y)"
      ],
      "metadata": {
        "id": "2cMGPMBoKDH5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 크로스 엔트로피\n",
        "#loss = nn.CrossEntropyLoss()\n",
        "#input = torch.randn(5, 6, requires_grad=True) # torch.randn은 평균이 0, 표준편차가 1인 가우시안 정규분포를 이용하여 숫자 생성\n",
        "#target = torch.empty(3, dtype=torch.long).random_(5) # torch.empty 는 torch.float32 랜덤값으로 채워진 텐서 반환\n",
        "#output = loss(input, target)\n",
        "#output.backward()"
      ],
      "metadata": {
        "id": "FX2FNTHtKDib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 드롭아웃\n",
        "class DropoutModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DropoutModel, self).__init__()\n",
        "        self.layer1 = torch.nn.Linear(784, 1200)\n",
        "        self.dropout1 = torch.nn.Dropout(0.5)    # 50%의 노드를 무작위로 선택하여 사용하지 않겠다는 의미\n",
        "        self.layer2 = torch.nn.Linear(1200, 1200)\n",
        "        self.dropout2 = torch.nn.Dropout(0.5)\n",
        "        self.layer3 = torch.nn.Linear(1200, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = self.dropout2(x)\n",
        "        return self.layer3(x)"
      ],
      "metadata": {
        "id": "l5aH6uLjKDkf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 미니배치 경사 하강법\n",
        "# class CustomDataset(Dataset):\n",
        "#    def __init__(self):\n",
        "#        self.x_data = [[1,2,3], [4,5,6], [7,8,9]]\n",
        "#        self.y_data = [[12], [18], [11]]\n",
        "#        def __len__(self):\n",
        "#            return len(self.x_data)\n",
        "#        def __getitem__(self, idx):\n",
        "#            x = torch.FloatTensor(self.x_data[idx])\n",
        "#            y = torch.FloatTensor(self.y_data[idx])\n",
        "#            return x, y\n",
        "#dataset = CustomDataset()\n",
        "#dataloader = DataLoader(dataset,       # 데이터셋\n",
        "#                        batch_size=2,  # 미니 배치 크기로 2의 제곱수를 사용하겠다는 의미\n",
        "#                        shuffle=True)  # 데이터를 불러올 때마다 랜덤으로 섞어서 가져옴"
      ],
      "metadata": {
        "id": "Wrb7ihYAKDmi"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}